---
title: "BACS HW11"
author: '109090046'
date: "2023-04-25"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

```{r}
auto <- read.table("D:/下載/auto-data.txt", header=FALSE, na.strings = "?") 
names(auto) <- c("mpg", "cylinders", "displacement", "horsepower", "weight", "acceleration", "model_year", "origin", "car_name")
```

## Question 1)

Let's deal with non-linearity first. Create a new dataset that log-transforms several variables from our original dataset (called cars in this case):

```{r}
cars_log <- with(auto, data.frame(log(mpg), log(cylinders), log(displacement), 
                                  log(horsepower), log(weight), log(acceleration),
                                  model_year, origin))
```

### a.

Run a new regression on the `cars_log dataset`, with `mpg.log.` dependent on all other variables

```{r}
regr_log <- lm(log.mpg. ~ log.cylinders. + log.displacement. + log.horsepower. + log.weight. + log.acceleration. + model_year + factor(origin), data = cars_log)
summary(regr_log)
```

#### i.

Which log-transformed factors have a significant effect on log.mpg. at 10% significance?

**ANS:** Variables with p-values less than 0.1 are significant at the 10% level. So `log.horsepower.`, `log.weight.`, `log.acceleration.` and `model_year` are significant at the 10% level.

#### ii.

Do some new factors now have effects on mpg, and why might this be?

**ANS:** Yes, `acceleration.` and `horsepower.`are new factors, and because the log transformation may have linearized the relationships between mpg and those factors. This makes it easier for the linear regression model to capture the relationships.

#### iii.

Which factors still have insignificant or opposite (from correlation) effects on mpg? Why might this be?

**ANS:** `cylinder` still has insignificant effects on mpg. Because insignificant factors might not have a strong relationship with mpg or their relationship may still be nonlinear even after the log transformation. Also, the presence of multi-collinearity between independent variables can lead to unstable estimates of the coefficients, making it difficult to interpret the results. Factors with opposite signs compared to the correlation might be due to suppression effects or interactions between variables that were not accounted for in the model.

### b.

Let's take a closer look at weight, because it seems to be a major explanation of mpg

#### i.

Create a regression (call it `regr_wt`) of `mpg` over weight from the original cars dataset

```{r}
regr_wt <- lm(mpg ~ weight, data = auto)
```

#### ii.

Create a regression (call it `regr_wt_log`) of `log.mpg.` on `log.weight.` from cars_log

```{r}
regr_wt_log <- lm(log.mpg. ~ log.weight., data = cars_log)
```

#### iii.

Visualize the residuals of both regression models (raw and log-transformed):

##### 1.

density plots of residuals
```{r}
residuals_raw <- data.frame(residuals = regr_wt$residuals, type = "Raw")
residuals_log <- data.frame(residuals = regr_wt_log$residuals, type = "Log-transformed")
residuals_combined <- rbind(residuals_raw, residuals_log)
```

```{r}
density_residual_raw <- ggplot(residuals_raw, aes(x = residuals, fill = type)) +
  geom_density(alpha = 0.5) +
  theme_grey() +
  labs(title = "Density Plots of Residual (Raw)", x = "Residual", y = "Density")

density_residual_raw
```

```{r}
density_residual_log <- ggplot(residuals_log, aes(x = residuals, fill = type)) +
  geom_density(alpha = 0.5) +
  theme_grey() +
  labs(title = "Density Plots of Residual (Log)", x = "Residual", y = "Density")

density_residual_log
```

```{r}
density_residuals <- ggplot(residuals_combined, aes(x = residuals, fill = type)) +
  geom_density(alpha = 0.5) +
  theme_grey() +
  labs(title = "Density Plots of Residuals", x = "Residuals", y = "Density")

density_residuals
```

##### 2.

scatterplot of log.weight. vs. residuals

```{r}
scatter_residuals <- ggplot(cars_log, aes(x = log.weight., y = regr_wt_log$residuals)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  theme_grey() +
  labs(title = "Scatterplot of Log(Weight) vs. Residuals", x = "Log(Weight)", y = "Residuals")

scatter_residuals
```

#### iv.

Which regression produces better distributed residuals for the assumptions of regression?

**ANS:** Compare the density plots of residuals for both regression models. Both raw and log-transformed one are normally distributed and centered to zero, so I think it's hard to tell. But by the comparing graphic, we can see that log-transformed is more centralized in the center.

#### v.

How would you interpret the slope of log.weight. vs log.mpg. in simple words?

**ANS:** The slope of `log.weight.` vs `log.mpg.` represents the percentage change in mpg for a 1% increase in weight. In other words, if the weight of a car increases by 1%, the miles per gallon will change by the percentage indicated by the slope.

#### vi.

From its standard error, what is the 95% confidence interval of the slope of log.weight. vs log.mpg.?

```{r}
confint(regr_wt_log, level = 0.95)["log.weight.",]
```

------------------------------------------------------------------------

## Question 2)

Let's tackle multi-collinearity next. Consider the regression model:

```{r}
regr_log <- lm(log.mpg. ~ log.cylinders. + log.displacement. + log.horsepower. +
                              log.weight. + log.acceleration. + model_year +
                              factor(origin), data=cars_log)
```

### a.

Using regression and R2, compute the VIF of log.weight. using the approach shown in class

```{r}
log_wei_regr <- lm(log.weight. ~ log.mpg. + log.displacement. + log.horsepower. +
                              log.cylinders. + log.acceleration. + model_year +
                              factor(origin), data=cars_log, na.action = na.exclude)

r2_log_wei <- summary(log_wei_regr)$r.squared
vif_log_wei <- 1 / (1 - r2_log_wei)

vif_log_wei
```

### b.

Let's try a procedure called Stepwise VIF Selection to remove highly collinear predictors. Start by Installing the 'car' package in RStudio -- it has a function called vif() (note: CAR package stands for Companion to Applied Regression -- it isn't about cars!)

```{r}
library(car)
```

#### i.

Use vif(regr_log) to compute VIF of the all the independent variables

```{r}
vif(regr_log)
```

#### ii.

Eliminate from your model the single independent variable with the largest VIF score that is also greater than 5

```{r}
regr_vif <- regr_log
vif_scores <- vif(regr_vif)
worst_var <- row.names(vif_scores)[which.max(vif_scores)]

if (max(vif_scores) > 5) {
  regr_log_elim <- update(regr_log, as.formula(paste("~ . -", worst_var)))
}

summary(regr_log_elim)
```

#### iii.

Repeat steps (i) and (ii) until no more independent variables have VIF scores above 5

```{r}
vif_scores <- vif(regr_log_elim)

while (max(vif_scores) > 5) {
  worst_var <- row.names(vif_scores)[which.max(vif_scores)]
  regr_log_elim <- update(regr_log_elim, as.formula(paste("~ . -", worst_var)))
  vif_scores <- vif(regr_log_elim)
}
```

#### iv.

Report the final regression model and its summary statistics

```{r}
summary(regr_log_elim)
```

### c.

Using stepwise VIF selection, have we lost any variables that were previously significant?\
If so, how much did we hurt our explanation by dropping those variables? (hint: look at model fit)

```{r}
summary(regr_log)
summary(regr_log_elim)
```

**ANS:** Comparing the result of two summary. Yes, we have lost `log.horsepower.` that was previously significant. To assess the impact of dropping those variables on the overall model fit, compare the R-squared or adjusted R-squared values. The Adjusted R-squared value drops from 0.8897 to 0.8841, which indicate that dropping these variables did not hurt the model's explanatory power much.

### d.

From only the formula for VIF, try deducing/deriving the following:

#### i.

If an independent variable has no correlation with other independent variables, what would its VIF score be?

**ANS:**

If an independent variable has no correlation with other independent variables, it means that it cannot be explained by the other variables, which results in an R² value of 0. Using the VIF formula, we get: VIF = 1 / (1 - 0) = 1

So, the VIF score would be 1.

#### ii.

Given a regression with only two independent variables (X1 and X2), how correlated would X1 and X2 have to be, to get VIF scores of 5 or higher? To get VIF scores of 10 or higher?

**ANS:**

Given a regression with only two independent variables (X1 and X2), we can use the VIF formula to find the correlation required to get VIF scores of 5 or higher and 10 or higher.

For VIF = 5:

5 = 1 / (1 - R²)

Solve for R²:

R² = 1 - (1 / 5) = 0.8

For VIF = 10:

10 = 1 / (1 - R²)

Solve for R²:

R² = 1 - (1 / 10) = 0.9

Now, since there are only two independent variables (X1 and X2), their R² is equivalent to the square of their correlation coefficient (r²). So, we need to find r for both cases:

For R² = 0.8:

r = sqrt(0.8) ≈ 0.894

For R² = 0.9:

r = sqrt(0.9) ≈ 0.949

So, X1 and X2 need to be correlated with approximately 0.894 or higher to get VIF scores of 5 or higher, and approximately 0.949 or higher to get VIF scores of 10 or higher.

---

## Question 3)
Might the relationship of weight on mpg be different for cars from different origins? 
Let’s try visualizing this. First, plot all the weights, using different colors and symbols for the three origins:

### a.
Let’s add three separate regression lines on the scatterplot, one for each of the origins.
```{r}
origin_colors = c("skyblue", "darkgreen", "coral")
ggplot(cars_log, aes(x=log.weight., y=log.mpg., color=factor(origin), shape=factor(origin))) +
  geom_point() +
  theme_grey() +
  geom_smooth(method="lm", se=FALSE, linetype="solid") +
  scale_color_manual(values=origin_colors) +
  scale_shape_manual(values=c(1, 2, 3)) +
  labs(title="Relationship between log(weight) and log(mpg) by origin",
       x="log(weight)",
       y="log(mpg)",
       color="Origin",
       shape="Origin")
```

### b. 
[not graded] Do cars from different origins appear to have different weight vs. mpg relationships?

**ANS:** The slope of the regression line are different, but they are close. So, we can conclude that they have same relationship that as mpg increase, weight decrease, vic versa.