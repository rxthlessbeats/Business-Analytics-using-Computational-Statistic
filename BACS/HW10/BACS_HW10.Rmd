---
title: "BACS HW10"
author: '109090046'
date: "2023-04-20"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(magrittr)
library(tidyverse)
```

## Question 1)

We will use the `interactive_regression()` function from `CompStatsLib` again -- Windows users please make sure your desktop scaling is set to 100% and RStudio zoom is 100%; alternatively, run R from the Windows Command Prompt. To answer the questions below, understand each of these four scenarios by simulating them:

Scenario 1: Consider a very narrowly dispersed set of points that have a negative or positive steep slope

Scenario 2: Consider a widely dispersed set of points that have a negative or positive steep slope

Scenario 3: Consider a very narrowly dispersed set of points that have a negative or positive shallow slope

Scenario 4: Consider a widely dispersed set of points that have a negative or positive shallow slope

### a.

Comparing scenarios 1 and 2, which do we expect to have a stronger R2?

**ANS:** We would expect Scenario 1 to have a stronger R2. In Scenario 1, the points are very narrowly dispersed around a steep slope, indicating a tight linear relationship between the variables. A higher R2 value represents a better fit of the regression line to the data points, and since the points are more closely clustered in Scenario 1, we can expect a higher R2 value.

### b.

Comparing scenarios 3 and 4, which do we expect to have a stronger R2?

**ANS:** We would expect Scenario 3 to have a stronger R2. Even though the slope is shallow in both scenarios, the points are more narrowly dispersed around the slope in Scenario 3. This indicates a stronger linear relationship between the variables, leading to a higher R2 value.

### c.

Comparing scenarios 1 and 2, which do we expect has bigger/smaller SSE, SSR, and SST? (intuitively)

**ANS:** Intuitively, we would expect Scenario 1 to have a smaller SSE (sum of squared errors) because the points are more tightly clustered around the regression line, resulting in smaller errors. SSR (sum of squares due to regression) is likely to be larger in Scenario 1 since the points are more tightly clustered and better explained by the regression line. SST (total sum of squares) is the sum of SSE and SSR, so it depends on the specific data sets, but we could expect Scenario 1 to have a smaller SST due to the smaller dispersion of data points.

### d.

Comparing scenarios 3 and 4, which do we expect has bigger/smaller SSE, SSR, and SST? (intuitively)

**ANS:** Intuitively, we would expect Scenario 3 to have a smaller SSE because the points are more tightly clustered around the regression line, resulting in smaller errors. SSR is likely to be larger in Scenario 3 since the points are more tightly clustered and better explained by the regression line. As for SST, similar to the comparison between scenarios 1 and 2, it depends on the specific data sets, but we could expect Scenario 3 to have a smaller SST due to the smaller dispersion of data points.

------------------------------------------------------------------------

## Question 2)

Let's analyze the `programmer_salaries.txt` dataset we saw in class. Read the file using `read.csv("programmer_salaries.txt", sep="\t")` because the columns are separated by tabs (`\t`).

### a.

Use the `lm()` function to estimate the regression model `Salary ~ Experience + Score + Degree` Show the beta coefficients, R2, and the first 5 values of y (`$fitted.values`) and (`$residuals`)

```{r}
programmer_salaries <- read.csv("D:/下載/programmer_salaries.txt", sep="\t")

model <- lm(Salary ~ Experience + Score + Degree, data=programmer_salaries)
```

```{r, echo=FALSE}
cat("Beta coefficients:\n")
print(model$coefficients)

# print the R-squared value
cat("\nR-squared:\n")
print(summary(model)$r.squared)

# print the first 5 fitted values
cat("\nFirst 5 fitted values:\n")
print(head(fitted.values(model), 5))

# print the first 5 residuals
cat("\nFirst 5 residuals:\n")
print(head(residuals(model), 5))
```

### b.

Use only linear algebra and the geometric view of regression to estimate the regression yourself:

#### i.

Create an X matrix that has a first column of 1s followed by columns of the independent variables(only show the code)

```{r}
X <- programmer_salaries %$% cbind(1, Experience, Score, Degree)
```

#### ii.

Create a y vector with the Salary values (only show the code)

```{r}
y <- programmer_salaries$Salary
```

#### iii.

Compute the beta_hat vector of estimated regression coefficients (show the code and values)

```{r}
beta_hat <- solve(t(X) %*% X) %*% (t(X) %*% y)
beta_hat
```

#### iv.

Compute a y_hat vector of estimated y values, and a res vector of residuals (show the code and the first 5 values of y_hat and res)

```{r}
y_hat <- X %*% beta_hat
res <- y - y_hat
head(y_hat, 5)
head(res, 5)
```

#### v.

Using only the results from (i) -- (iv), compute SSR, SSE and SST (show the code and values)

```{r}
SSR <- sum((y_hat - mean(y))^2)
SSE <- sum(res^2)
SST <- sum((y - mean(y))^2)
```

```{r, echo=FALSE}
cat("SSR is:", SSR, "\n")
cat("SSR is:", SSE, "\n")
cat("SSR is:", SST, "\n")
```

### c.

Compute R2 for in two ways, and confirm you get the same results (show code and values):

#### i.

Use any combination of SSR, SSE, and SST

```{r}
R2_1 <- SSR / SST
R2_1
```

#### ii.

Use the squared correlation of vectors y and y hat

```{r}
R2_2 <- cor(y, y_hat)^2
R2_2
```

------------------------------------------------------------------------

## Question 3)

We're going to take a look back at the early heady days of global car manufacturing, when American, Japanese, and European cars competed to rule the world. Take a look at the data set in file `auto-data.txt`. We are interested in explaining what kind of cars have higher fuel efficiency (mpg).

1.  mpg: miles-per-gallon (dependent variable)

2.  cylinders: cylinders in engine

3.  displacement: size of engine

4.  horsepower: power of engine

5.  weight: weight of car

6.  acceleration: acceleration ability of car

7.  model_year: year model was released

8.  origin: place car was designed (1: USA, 2: Europe, 3: Japan)

9.  car_name: make and model names

Note that the data has missing values ('?' in data set), and lacks a header row with variable names: 
```{r}
auto <- read.table("D:/下載/auto-data.txt", header=FALSE, na.strings = "?") 
names(auto) <- c("mpg", "cylinders", "displacement", "horsepower", "weight", "acceleration", "model_year", "origin", "car_name")
```

### a.
Let’s first try exploring this data and problem:

#### i.
Visualize the data as you wish (report only relevant/interesting plots)
```{r}
library(patchwork)
p1 <- ggplot(auto, aes(x = weight, y = mpg)) +
  geom_point() +
  theme_grey() +
  labs(title = "MPG vs. Weight", x = "Weight", y = "Miles per Gallon")

p2 <- ggplot(auto, aes(x = horsepower, y = mpg)) +
  geom_point() +
  theme_grey() +
  labs(title = "MPG vs. Horsepower", x = "Horsepower", y = "Miles per Gallon")

p3 <- ggplot(auto, aes(x = displacement, y = mpg)) +
  geom_point() +
  theme_grey() +
  labs(title = "MPG vs. Displacement", x = "Displacement", y = "Miles per Gallon")

p4 <- ggplot(auto, aes(x = model_year, y = mpg)) +
  geom_point() +
  theme_grey() +
  labs(title = "MPG vs. Model Year", x = "Model Year", y = "Miles per Gallon")

(p1 | p2) / (p3 | p4)
```

#### ii.
Report a correlation table of all variables, rounding to two decimal places
(in the cor() function, set use="pairwise.complete.obs" to handle missing values)
```{r}
cor_table <- cor(auto[,1:8], use = "pairwise.complete.obs")
round(cor_table, 2)
```
#### iii.
From the visualizations and correlations, which variables appear to relate to mpg?

**ANS:** From the plots and correlation table, we can see that `weight`, `displacement`, `horsepower`, and `cylinders` have strong negative correlations with `mpg`. `Acceleration`, `model_year`, `origin` have weak positive correlations.

#### iv.
Which relationships might not be linear? (don’t worry about linearity for rest of this HW)

**ANS:** `acceleration` and `model_year` might not be linear with `origin`.

#### v.
Are there any pairs of independent variables that are highly correlated (r > 0.7)?

**ANS:** Yes, such as:
`displacement` and `cylinders`, `horsepower` and `cylinders`, `weight` and `cylinders`,
`displacement` and `horsepower`, `displacement` and `weight`, `horsepower` and `weight`.

### b.
Let’s create a linear regression model where mpg is dependent upon all other suitable variables (Note: origin is categorical with three levels, so use `factor(origin)` in `lm(...)`  to split it into two dummy variables)
```{r}
regr <- lm(mpg ~ cylinders + displacement + horsepower + weight + acceleration + model_year + factor(origin), data = auto)

summary(regr)
```

#### i.
Which independent variables have a ‘significant’ relationship with mpg at 1% significance?

**ANS:** `displacement`, `acceleration`, `model_year` have significant relationship with mpg at 1% significance, since p-values less than 0.01.

#### ii.
Looking at the coefficients, is it possible to determine which independent variables are the most effective at increasing mpg? If so, which ones, and if not, why not? (hint: units!)

**ANS:** It's difficult to compare the coefficients directly since they have different units. Standardizing the variables can help with comparison.

### c.
Let’s try to resolve some of the issues with our regression model above.

#### i.
Create fully standardized regression results: are these slopes easier to compare?
(note: consider if you should standardize origin)
```{r}
auto_standardized <- auto
auto_standardized[,1:7] <- scale(auto[,1:7])
regr_standardized <- lm(mpg ~ cylinders + displacement + horsepower + weight + acceleration + model_year + factor(origin), data = auto_standardized)
summary(regr_standardized)
```

**ANS:** Yes, the slopes of the fully standardized regression results are easier to compare. When we standardize the variables, we put them on the same scale (mean = 0, standard deviation = 1), making it easier to interpret the effect of each variable on the dependent variable. This allows us to compare the relative importance of each independent variable in the regression model.

#### ii.
Regress mpg over each non-significant independent variable, individually.
Which ones become significant when we regress mpg over them individually?
```{r}
regr_cyl <- lm(mpg ~ cylinders, data = auto)
summary(regr_cyl)

regr_hor <- lm(mpg ~ horsepower, data = auto)
summary(regr_hor)

regr_acc <- lm(mpg ~ acceleration, data = auto)
summary(regr_acc)
```

**ANS:** `acceleration` becomes significant because the p-value is 0.0154, larger than 0.01.

#### iii.
Plot the distribution of the residuals: are they normally distributed and centered around zero?
(get the residuals of a fitted linear model, e.g. `regr <- lm(...)`, using `regr$residuals`
```{r}
residuals_hist <- ggplot(data.frame(residuals = regr$residuals), aes(residuals)) +
  geom_histogram(fill = "steelblue", color = "white", bins = 30) +
  theme_minimal() +
  labs(title = "Distribution of Residuals", x = "Residuals", y = "Count")

residuals_hist
```

**ANS:** Yes, they are normally distributed and centered around zero.

